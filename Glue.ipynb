{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init(\"C:\\Hadoop\\spark-3.0.0-bin-hadoop3.2\")\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('Training').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when,udf,lit,split,current_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sources = {}\n",
    "rules = {}\n",
    "targets = {}\n",
    "\n",
    "load = csv.DictReader(open(\"dl/sources.csv\"))\n",
    "for x in load:\n",
    "    sources[x['source_var']] = x\n",
    "    \n",
    "load = csv.DictReader(open(\"dl/rules.csv\"))\n",
    "for x in load:\n",
    "    rules[x['rule_name']] = x\n",
    "    \n",
    "load = csv.DictReader(open(\"dl/targets.csv\"))\n",
    "for x in load:\n",
    "    targets[x['target_var']] = x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setNewValue(severity,df,oldColumn,newColumn,default):\n",
    "    \n",
    "    if severity.upper() == 'CRITICAL':\n",
    "        interupt = df.filter(df[newColumn]==False).count()\n",
    "        if interupt:\n",
    "            sys.exit()\n",
    "    elif severity.upper() == 'NONE':\n",
    "        result = df\n",
    "        return result\n",
    "    \n",
    "    result = df.withColumn('severity', when(df[newColumn],'Success').otherwise(severity))\n",
    "    result = result.withColumn(newColumn, when(df[newColumn],df[oldColumn]).otherwise(default))\n",
    "    \n",
    "    return result\n",
    "\n",
    "def checkRegex(name,df,column,exp):\n",
    "    temp = df.withColumn('ptemp', df[column])\n",
    "    result = temp.withColumn(name, temp.ptemp.rlike(exp)).drop('ptemp')\n",
    "    \n",
    "    return result\n",
    "\n",
    "def checkLink(name,df1,column1,df2,column2):\n",
    "    \n",
    "    # Rename the column in the second dataframe to avoid ambiguity\n",
    "    temp = df2.withColumnRenamed(column2,'ptemp').select('ptemp')\n",
    "    \n",
    "    # Execute the left join to check links\n",
    "    case = df1.join(temp, df1[column1] == temp.ptemp, 'left')\n",
    "    \n",
    "    # Create the column with true or false argument according to null values in left join\n",
    "    result = case.withColumn(name, when(case.ptemp.isNull(),False).otherwise(True)).drop('ptemp')\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Define if the value is 4 numerical digits\n",
    "def isYear(arg):\n",
    "    if re.match('\\d\\d\\d\\d$', arg):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "# Define if the value is a number in a 1-31 range    \n",
    "def isDay(arg):    \n",
    "    if re.match('^\\d{1,2}$', arg):\n",
    "        if int(arg)>0 and int(arg)<=31:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "# Define if the value is considerable as a month and return it in a numerical format\n",
    "def isMonth(arg):\n",
    "    months = ['JANUARY','FEBRUARY','MARCH','APRIL','MAY','JUNE','JULY','SEPTEMBER','OCTOBER','NOVEMBER','DECEMBER']\n",
    "    \n",
    "    #Check if the month is in 99 format and between 1-12\n",
    "    if re.match('^\\d{1,2}$', arg):\n",
    "        if int(arg)>=1 and int(arg)<=12:\n",
    "            return arg\n",
    "        else:\n",
    "            return arg\n",
    "    \n",
    "    #Check if the month correspond to one of the preset values\n",
    "    elif any(re.match('^'+arg.upper(),month) for month in months):\n",
    "        month = next(i+1 for i,month in enumerate(months) if re.match('^'+arg.upper(),month))\n",
    "        return \"{0:0=2d}\".format(month)\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def getDate(row):    \n",
    "    if isYear(row[0]) and isMonth(row[1]) and isDay(row[2]):\n",
    "        return row[0]+'-'+str(isMonth(row[1]))+'-'+row[2]\n",
    "        \n",
    "    elif isDay(row[0]) and isMonth(row[1]) and isYear(row[2]):\n",
    "        return row[2]+'-'+str(isMonth(row[1]))+'-'+row[0]\n",
    "        \n",
    "    elif isMonth(row[0]) and isDay(row[1]) and isYear(row[2]):\n",
    "        return row[2]+'-'+str(isMonth(row[0]))+'-'+row[1]\n",
    "        \n",
    "    else:\n",
    "        return None\n",
    "getDateUDF = udf(getDate)\n",
    "\n",
    "def transformDateFormat(name,df,column):\n",
    "    case = split(column,'\\W').alias(name)\n",
    "    \n",
    "    result = df.withColumn(name, getDateUDF(case))\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_source = {}\n",
    "df = {}\n",
    "\n",
    "# Load Dataframes\n",
    "for source in sources:\n",
    "    path = sources[source]['path_source'].split(',')\n",
    "    table_source = sources[source]['table_source']\n",
    "    \n",
    "    if table_source not in df_source:\n",
    "        df_source[table_source] = spark.read.csv(path, header=True, inferSchema=True)\n",
    "        \n",
    "    df[sources[source]['source_var']] = df_source[table_source]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load rules\n",
    "for rule in rules:\n",
    "    \n",
    "    rule_name = rules[rule]['rule_name']\n",
    "    source_var = rules[rule]['source_var']\n",
    "    function_name = rules[rule]['func_name']\n",
    "    arg = rules[rule]['arg']\n",
    "    default = rules[rule]['default_value']\n",
    "    severity = rules[rule]['severity']\n",
    "    target_var = rules[rule]['target_var']\n",
    "    dq_dim = rules[rule]['dq_dim']\n",
    "    column = sources[source_var]['column_source']\n",
    "    dataFrame = df[source_var]\n",
    "    \n",
    "    if function_name == 'checkRegex':\n",
    "        \n",
    "        df[target_var] = checkRegex(rule_name, dataFrame, column, arg)\n",
    "        \n",
    "    elif function_name == 'checkLink':\n",
    "        \n",
    "        dataFrame2 = df[arg]\n",
    "        column2 = sources[arg]['column_source']\n",
    "        \n",
    "        df[target_var] = checkLink(rule_name,dataFrame,column,dataFrame2,column2)\n",
    "        \n",
    "    elif function_name == 'transformDateFormat':\n",
    "        \n",
    "        df[target_var] = transformDateFormat(rule_name,dataFrame,column)\n",
    "   \n",
    "    else: \n",
    "        break\n",
    "        \n",
    "    if target_var not in sources:\n",
    "        sources[target_var] = sources[source_var]\n",
    "        sources[target_var]['column_source'] = rule_name\n",
    "            \n",
    "    df[target_var] = setNewValue(severity,df[target_var],column,rule_name,default)\n",
    "    df[target_var] = df[target_var].withColumn('dq_dim', lit(dq_dim))\n",
    "    df[target_var] = df[target_var].withColumn('rule', lit(rule_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(car_id=1, car_brand='Volv 0', car_model='XC40', year='2019', rule4_2='2019', severity='Success', dq_dim='Conformity', rule='rule4_2')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['carDF_yr'].collect()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "locationDF_pc\n",
      "locationDF_pn\n",
      "locationDF_cn\n",
      "carDF_cb\n",
      "carDF_yr\n",
      "customer_locationDF_ci\n",
      "customer_locationDF_li\n",
      "customer_locationDF_new_tdt\n"
     ]
    }
   ],
   "source": [
    "for x in df:\n",
    "    if 'severity' in df[x].columns:\n",
    "        data_source = 'SALES'\n",
    "        print(x)\n",
    "        dataset = sources[x]['table_source']\n",
    "        key_nm = sources[x]['df_id']\n",
    "        field_nm = sources[x]['column_source']\n",
    "        \n",
    "        \n",
    "        df[x] = df[x].withColumn('processing_dttm', current_timestamp())\n",
    "        df[x] = df[x].withColumn('data_source', lit(data_source))\n",
    "        df[x] = df[x].withColumn('dataset', lit(dataset))\n",
    "        df[x] = df[x].withColumn('data_domain', lit(dataset))\n",
    "        df[x] = df[x].withColumn('key_nm', lit(key_nm))\n",
    "        df[x] = df[x].withColumn('key_val', df[x][key_nm])\n",
    "        df[x] = df[x].withColumn('field_nm', lit(field_nm))\n",
    "        df[x] = df[x].withColumn('field_value', df[x][field_nm])\n",
    "        df[x] = df[x].withColumn('dq_rule_result', df[x]['severity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|cust_id|\n",
      "+-------+\n",
      "|      1|\n",
      "|      2|\n",
      "|      3|\n",
      "|      4|\n",
      "|      5|\n",
      "|      6|\n",
      "|      7|\n",
      "|      8|\n",
      "|      9|\n",
      "|     10|\n",
      "|     11|\n",
      "|     12|\n",
      "|     13|\n",
      "|     14|\n",
      "|     15|\n",
      "|     16|\n",
      "|     17|\n",
      "|     18|\n",
      "|     19|\n",
      "|     20|\n",
      "+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df[x].select(df[x][key_nm]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|   rule5_1|\n",
      "+----------+\n",
      "|2019-01-21|\n",
      "|2020-01-19|\n",
      "|2019-02-27|\n",
      "|2019-09-26|\n",
      "|2019-05-07|\n",
      "|2019-05-17|\n",
      "|2020-03-26|\n",
      "|2019-10-01|\n",
      "|2019-10-03|\n",
      "|2020-01-12|\n",
      "|2019-06-19|\n",
      "|2019-07-27|\n",
      "|2019-05-03|\n",
      "|2019-05-31|\n",
      "|2020-03-22|\n",
      "|2020-01-12|\n",
      "|      null|\n",
      "|2019-01-27|\n",
      "|2019-06-25|\n",
      "|2019-05-29|\n",
      "+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df[x].select('rule5_1').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_frame_mapped = ApplyMapping.apply(frame = target_frame, mappings = [\n",
    "    (\"processing_dttm\",\"string\",\"processing_dttm\",\"string\"),\n",
    "    (\"data_source\",\"string\",\"data_source\",\"string\"),\n",
    "    (\"dataset\",\"string\",\"dataset\",\"string\"),\n",
    "    (\"data_domain\",\"string\",\"data_domain\",\"string\"),\n",
    "    (\"key_nm\",\"string\",\"key_nm\",\"string\"),\n",
    "    (key_nm,\"bigint\",\"key_value\",\"bigint\"),\n",
    "    (\"field_nm\",\"string\",\"field_nm\",\"string\"),\n",
    "    (field_nm,\"string\",\"field_val\",\"string\"),\n",
    "    (\"dq_dim\",\"string\",\"dq_dim\",\"string\"),\n",
    "    (\"Dq_rule_result\",\"string\",\"Dq_rule_result\",\"string\")\n",
    "], transformation_ctx = \"target_frame_mapped\")    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
